{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0151c3fa",
   "metadata": {},
   "source": [
    "Spark Connectivity (Please ensure python version 3.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40b3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948cfa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.1.1\n",
      "Application Name: CS5305 Spark Connection\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session connected to local Spark container\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CS5305 Spark Connection\") \\\n",
    "    .master(\"spark://localhost:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get Spark context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Issue some basic commands\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "\n",
    "# Display cluster information\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53298c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spark Environment Information ===\n",
      "Spark Version: 4.1.1\n",
      "Spark Home: D:\\git\\LUMS\\cs5305\\python310env\\Lib\\site-packages\\pyspark\\bin\\..\n",
      "Python Version: 3.10\n",
      "\n",
      "=== Cluster Information ===\n",
      "Master URL: spark://localhost:7077\n",
      "Application ID: app-20260118085431-0002\n",
      "Application Name: CS5305 Spark Connection\n",
      "Default Parallelism: 2\n",
      "Default Min Partitions: 2\n"
     ]
    }
   ],
   "source": [
    "# Query Spark environment and cluster information\n",
    "print(\"=== Spark Environment Information ===\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Home: {sc._jvm.System.getenv('SPARK_HOME')}\")\n",
    "print(f\"Python Version: {sc.pythonVer}\")\n",
    "\n",
    "print(\"\\n=== Cluster Information ===\")\n",
    "print(f\"Master URL: {sc.master}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Default Min Partitions: {sc.defaultMinPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978e7b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Spark Configuration ===\n",
      "spark.rdd.compress: True\n",
      "spark.hadoop.fs.s3a.vectored.read.min.seek.size: 128K\n",
      "spark.sql.artifact.isolation.enabled: false\n",
      "spark.app.startTime: 1768726471196\n",
      "spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Dio.netty.allocator.type=pooled -Dio.netty.handler.ssl.defaultEndpointVerificationAlgorithm=NONE --enable-native-access=ALL-UNNAMED\n",
      "spark.app.submitTime: 1768726471044\n",
      "spark.sql.warehouse.dir: file:/D:/git/LUMS/cs5305/env_setup/spark-warehouse\n",
      "spark.driver.host: 192.168.112.146\n",
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Dio.netty.allocator.type=pooled -Dio.netty.handler.ssl.defaultEndpointVerificationAlgorithm=NONE --enable-native-access=ALL-UNNAMED\n",
      "spark.executor.id: driver\n",
      "spark.submit.pyFiles: \n",
      "spark.driver.port: 54150\n",
      "spark.app.name: CS5305 Spark Connection\n",
      "spark.master: spark://localhost:7077\n",
      "spark.hadoop.fs.s3a.vectored.read.max.merged.size: 2M\n",
      "spark.app.id: app-20260118085431-0002\n",
      "spark.submit.deployMode: client\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.ui.showConsoleProgress: true\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Spark Configuration ===\")\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{item[0]}: {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5fccc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of data: 15\n"
     ]
    }
   ],
   "source": [
    "# Create a simple RDD to test the connection\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "print(f\"Sum of data: {rdd.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7313b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "+---+-------+\n",
      "\n",
      "Master: spark://localhost:7077\n",
      "Default Parallelism: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a simple DataFrame to test the connection\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")], [\"id\", \"name\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64caa88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
