# Start with Java 21
FROM eclipse-temurin:21-jdk-jammy

# Define Versions
ENV SPARK_VERSION=4.1.1
ENV HADOOP_VERSION=3
ENV DELTA_VERSION=4.0.1
ENV PYTHON_VERSION=3.14
ENV SCALA_VERSION=2.13
ENV HOME=/home/spark

USER root

# 1. Install Python 3.14 via deadsnakes PPA
#    Note: distutils was removed from stdlib in 3.12+; setuptools (installed by pip) provides it.
RUN apt-get update && apt-get install -y software-properties-common curl && \
    add-apt-repository ppa:deadsnakes/ppa -y && \
    apt-get update && \
    apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 && \
    curl -sS https://bootstrap.pypa.io/get-pip.py | python3 && \
    rm -rf /var/lib/apt/lists/*

# 2. Download and Install Spark 4.1.1
RUN curl -O https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt/ && \
    ln -s /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
    
# Install linux utilities
RUN apt-get update && apt-get install -y \
    vim \
    curl \
    wget \
    netcat-openbsd \
    net-tools \
    iputils-ping \
    && rm -rf /var/lib/apt/lists/*

# 3. Environment Setup
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin
ENV PYSPARK_PYTHON=python3

# 4. Install Delta Lake Python Bindings
RUN pip3 install --no-cache-dir \
        delta-spark==${DELTA_VERSION} \
        pyspark[connect]==${SPARK_VERSION} \
        grpcio \
        protobuf \
        jupyter \
        jupyterlab \
        ipykernel

WORKDIR /opt/spark/work-dir

# 4. Pre-download Connect & Delta JARs to the image
RUN /opt/spark/bin/spark-shell --packages \
    io.delta:delta-spark_2.13:${DELTA_VERSION},\
    io.delta:delta-connect-server_2.13:${DELTA_VERSION},\
    org.apache.spark:spark-connect_2.13:${SPARK_VERSION} \
    --repositories https://repo1.maven.org/maven2/
# Set default Spark mode
ENV SPARK_MODE=master

# Create startup script that checks SPARK_MODE environment variable
RUN mkdir -p /opt/spark/startup
COPY ./entrypoint.sh /opt/spark/startup/entrypoint.sh
RUN chmod +x /opt/spark/startup/entrypoint.sh

EXPOSE 4040 7077 8080 8888 15002

# Create spark user and grant ownership of Spark directories
RUN groupadd -r spark && useradd -r -g spark -m -d /home/spark -s /bin/bash spark && \
    chown -R spark:spark /opt/spark /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}

RUN mkdir -p /home/spark/.local/share/jupyter && \
        chown -R spark:spark /home/spark/.local/share/jupyter

USER spark

ENTRYPOINT ["/opt/spark/startup/entrypoint.sh"]
