{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9aecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully!\n",
      "Spark version: 4.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/07 07:05:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DeltaLakeExample\") \\\n",
    "      \n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf2b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table created successfully!\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Starter\"), (2, \"Pro\")], [\"id\", \"plan\"])\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-tables/table2\")\n",
    "print(\"Delta table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610ae189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reading Delta Table ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/07 07:05:56 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   plan|\n",
      "+---+-------+\n",
      "|  1|Starter|\n",
      "|  2|    Pro|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Read the Delta table we created\n",
    "print(\"=== Reading Delta Table ===\")\n",
    "delta_df = spark.read.format(\"delta\").load(\"/tmp/delta-tables/table2\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6cd4ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Table Schema ===\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- plan: string (nullable = true)\n",
      "\n",
      "\n",
      "=== Table Count ===\n",
      "Number of records: 2\n"
     ]
    }
   ],
   "source": [
    "# 2. Check table schema and metadata\n",
    "print(\"\\n=== Table Schema ===\")\n",
    "delta_df.printSchema()\n",
    "print(\"\\n=== Table Count ===\")\n",
    "print(f\"Number of records: {delta_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727f8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Appending New Data ===\n",
      "Updated table:\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "|  1|   Starter|\n",
      "|  4|     Basic|\n",
      "|  2|       Pro|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Add more data (Append mode)\n",
    "print(\"\\n=== Appending New Data ===\")\n",
    "new_data = spark.createDataFrame([(3, \"Enterprise\"), (4, \"Basic\")], [\"id\", \"plan\"])\n",
    "new_data.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta-tables/table2\")\n",
    "\n",
    "# Read updated table\n",
    "updated_df = spark.read.format(\"delta\").load(\"/tmp/delta-tables/table2\")\n",
    "print(\"Updated table:\")\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e37006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Updating Records ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/07 07:07:17 WARN UpdateCommand: Could not validate number of records due to missing statistics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After update:\n",
      "+---+-----------+\n",
      "| id|       plan|\n",
      "+---+-----------+\n",
      "|  1|Starter Pro|\n",
      "|  3| Enterprise|\n",
      "|  4|      Basic|\n",
      "|  2|        Pro|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"/tmp/delta-tables/table2\")\n",
    "\n",
    "# 5. Update existing records\n",
    "print(\"\\n=== Updating Records ===\")\n",
    "deltaTable.update(\n",
    "    condition=\"id = 1\",\n",
    "    set={\"plan\": \"'Starter Pro'\"}\n",
    ")\n",
    "\n",
    "# Read after update\n",
    "print(\"After update:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta-tables/table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ed489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Deleting Records ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/04 06:12:33 WARN DeleteCommand: Could not validate number of records due to missing statistics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After delete:\n",
      "+---+-----------+\n",
      "| id|       plan|\n",
      "+---+-----------+\n",
      "|  1|Starter Pro|\n",
      "|  3| Enterprise|\n",
      "|  2|        Pro|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Delete records\n",
    "print(\"\\n=== Deleting Records ===\")\n",
    "deltaTable.delete(\"id = 4\")\n",
    "\n",
    "# Read after delete\n",
    "print(\"After delete:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta-tables/table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10052932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Upsert/Merge Operation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/04 06:12:34 WARN MapPartitionsRDD: RDD 206 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merge:\n",
      "+---+-----------+\n",
      "| id|       plan|\n",
      "+---+-----------+\n",
      "|  1|Starter Pro|\n",
      "|  3| Enterprise|\n",
      "|  2|    Pro Max|\n",
      "|  5|    Premium|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Upsert (Merge) operation\n",
    "print(\"\\n=== Upsert/Merge Operation ===\")\n",
    "merge_data = spark.createDataFrame([(2, \"Pro Max\"), (5, \"Premium\")], [\"id\", \"plan\"])\n",
    "\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    merge_data.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdate(set={\"plan\": \"source.plan\"}).whenNotMatchedInsert(values={\"id\": \"source.id\", \"plan\": \"source.plan\"}).execute()\n",
    "\n",
    "print(\"After merge:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta-tables/table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09da5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Time Travel Queries ===\n",
      "Version 0:\n",
      "+---+-------+\n",
      "| id|   plan|\n",
      "+---+-------+\n",
      "|  1|Starter|\n",
      "|  2|    Pro|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Time travel queries\n",
    "print(\"\\n=== Time Travel Queries ===\")\n",
    "# Read table at version 0\n",
    "print(\"Version 0:\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-tables/table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a55b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Table State ===\n",
      "+---+-----------+\n",
      "| id|       plan|\n",
      "+---+-----------+\n",
      "|  1|Starter Pro|\n",
      "|  3| Enterprise|\n",
      "|  2|    Pro Max|\n",
      "|  5|    Premium|\n",
      "+---+-----------+\n",
      "\n",
      "+-------+--------------------+---------+\n",
      "|version|           timestamp|operation|\n",
      "+-------+--------------------+---------+\n",
      "|     13|2026-02-04 06:12:...|    MERGE|\n",
      "|     12|2026-02-04 06:12:...|   DELETE|\n",
      "|     11|2026-02-04 06:12:...|   UPDATE|\n",
      "|     10|2026-02-04 06:12:...|    WRITE|\n",
      "|      9|2026-02-04 06:12:...|    WRITE|\n",
      "|      8|2026-02-04 06:10:...|    MERGE|\n",
      "|      7|2026-02-04 06:10:...|   DELETE|\n",
      "|      6|2026-02-04 06:10:...|   UPDATE|\n",
      "|      5|2026-02-04 06:10:...|    WRITE|\n",
      "|      4|2026-02-04 06:10:...|    WRITE|\n",
      "|      3|2026-02-04 06:09:...|    WRITE|\n",
      "|      2|2026-02-04 06:09:...|    WRITE|\n",
      "|      1|2026-02-04 06:09:...|    WRITE|\n",
      "|      0|2026-02-04 06:08:...|    WRITE|\n",
      "+-------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Table State ===\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta-tables/table2\").show()\n",
    "deltaTable.history().select(\"version\", \"timestamp\", \"operation\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
