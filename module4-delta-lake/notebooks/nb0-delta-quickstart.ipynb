{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27326eb-b153-455b-aac7-e1fef5b265c6",
   "metadata": {},
   "source": [
    "## Write to and read from a Delta Lake table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303e34b-0ebf-4f24-b1cb-be39cea040b9",
   "metadata": {},
   "source": [
    "### Write a Spark DataFrame to a Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55cf5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../common_notebooks/setup_spark_connection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83cfdca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = \"/opt/spark/delta-tables/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df98ff82-9a19-4744-94ca-0d1e2050c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "\n",
    "(data\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .save(delta_path + \"table1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae7e09-295c-4032-b4b5-8fca0f90f334",
   "metadata": {},
   "source": [
    "### Read the above Delta Lake table to a Spark DataFrame and display the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e74258ca-e9fd-4d6a-81da-7100a1836c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "        .read\n",
    "        .format(\"delta\")\n",
    "        .load(delta_path + \"table1\")\n",
    "        .orderBy(\"id\")\n",
    "      )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3a41b-b2bf-4754-b3d9-998fb00379db",
   "metadata": {},
   "source": [
    "## Overwrite a Delta Lake table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1104e6-98e8-4401-b7c7-007de71b6f91",
   "metadata": {},
   "source": [
    "### Overwrite the Delta Lake table written in the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92beaac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.range(5, 10)\n",
    "\n",
    "view_data = data.createOrReplaceTempView(\"data_view\")\n",
    "table2 = delta_path + \"table1\"\n",
    "spark.sql(f\"\"\"\n",
    "  MERGE INTO delta.`{table2}` AS target\n",
    "  USING data_view AS source\n",
    "  ON target.id = source.id\n",
    "  WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5297c-f6b4-41e0-8014-54d6958ea352",
   "metadata": {},
   "source": [
    "### Read the above overwritten Delta Lake table to a Spark DataFrame and display the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca38e5f9-077b-4fb8-8d3f-74d6c12b554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "        .read\n",
    "        .format(\"delta\")\n",
    "        .load(delta_path + \"table1\")\n",
    "        .orderBy(\"id\")\n",
    "      )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b416c8-6020-47ac-a017-3a8858a74d8b",
   "metadata": {},
   "source": [
    "## Delta Lake and [ACID](https://en.wikipedia.org/wiki/ACID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed33f55e-cfa2-4a09-a95d-0238b8f5d9e0",
   "metadata": {},
   "source": [
    "### Update Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1fec5f6-1aaa-4bc3-a2fa-b357e6bb6c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_path + \"table1\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  UPDATE delta.`{delta_path}table1`\n",
    "  SET id = id + 100\n",
    "  WHERE id % 2 == 0\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7116d795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "vacuum delta.`/opt/spark/delta-tables/table1`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a2e340d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        optimize delta.`/opt/spark/delta-tables/table1`\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c130b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "          alter table delta.`/opt/spark/delta-tables/table1`\n",
    "          set tblproperties ('enableChangeDataCapture' = 'true'\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "717e2070",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidPlanInput",
     "evalue": "No handler found for extension\n\nJVM stacktrace:\norg.apache.spark.sql.connect.common.InvalidPlanInput\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelationPlugin$3(SparkConnectPlanner.scala:253)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelationPlugin(SparkConnectPlanner.scala:253)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:235)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:133)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformSort(SparkConnectPlanner.scala:2249)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:163)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:133)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:306)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:150)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:74)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:314)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:225)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidPlanInput\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdelta_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m table_view \u001b[38;5;241m=\u001b[39m delta_table\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta_table_view\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m  select * from table_view\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m  order by id\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\akhawaja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\dataframe.py:1119\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\akhawaja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\dataframe.py:872\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    857\u001b[0m             errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    858\u001b[0m             messageParameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             },\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    864\u001b[0m table, _ \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mShowString\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_truncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mas_py()\n",
      "File \u001b[1;32mc:\\Users\\akhawaja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\dataframe.py:1794\u001b[0m, in \u001b[0;36mDataFrame._to_table\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_to_table\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpa.Table\u001b[39m\u001b[38;5;124m\"\u001b[39m, Optional[StructType]]:\n\u001b[0;32m   1793\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[1;32m-> 1794\u001b[0m     table, schema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execution_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (table, schema)\n",
      "File \u001b[1;32mc:\\Users\\akhawaja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:925\u001b[0m, in \u001b[0;36mSparkConnectClient.to_table\u001b[1;34m(self, plan, observations)\u001b[0m\n\u001b[0;32m    923\u001b[0m req \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_plan_request_with_metadata()\n\u001b[0;32m    924\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mCopyFrom(plan)\n\u001b[1;32m--> 925\u001b[0m table, schema, metrics, observed_metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
      "File \u001b[1;32mc:\\Users\\akhawaja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1560\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[1;34m(self, req, observations, self_destruct)\u001b[0m\n\u001b[0;32m   1557\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[1;32m-> 1560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n\u001b[0;32m   1561\u001b[0m         req, observations, progress\u001b[38;5;241m=\u001b[39mprogress\n\u001b[0;32m   1562\u001b[0m     ):\n\u001b[0;32m   1563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[0;32m   1564\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[1;32mc:\\Users\\akhawaja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1537\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[1;34m(self, req, observations, progress)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m-> 1537\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhawaja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1811\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[1;34m(self, error)\u001b[0m\n\u001b[0;32m   1809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[1;32m-> 1811\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\akhawaja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1882\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[1;34m(self, rpc_error)\u001b[0m\n\u001b[0;32m   1879\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrorClass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_HANDLE.SESSION_CHANGED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1880\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1882\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[0;32m   1883\u001b[0m                 info,\n\u001b[0;32m   1884\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[0;32m   1885\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[0;32m   1886\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[0;32m   1887\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mInvalidPlanInput\u001b[0m: No handler found for extension\n\nJVM stacktrace:\norg.apache.spark.sql.connect.common.InvalidPlanInput\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelationPlugin$3(SparkConnectPlanner.scala:253)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelationPlugin(SparkConnectPlanner.scala:253)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:235)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:133)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformSort(SparkConnectPlanner.scala:2249)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:163)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:133)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:306)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:150)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:74)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:314)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:225)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)"
     ]
    }
   ],
   "source": [
    "delta_table.toDF().orderBy(\"id\").show()\n",
    "table_view = delta_table.createOrReplaceTempView(\"delta_table_view\")\n",
    "spark.sql(\"\"\"\n",
    "  select * from table_view\n",
    "  order by id\n",
    "\"\"\")\n",
    "table_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8561cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "  UPDATE delta.`{delta_path}table2`\n",
    "  SET id = id + 100\n",
    "  WHERE id % 2 == 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06375c77-855d-4230-8d5f-4a13feabd711",
   "metadata": {},
   "source": [
    "###  `delete`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69423a2-b8b5-4e77-8d7c-492571e3f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete every even value\n",
    "(delta_table\n",
    "  .delete(\n",
    "    condition = expr(\"id % 2 == 0\")\n",
    "  )\n",
    ")\n",
    "\n",
    "(delta_table\n",
    "  .toDF()\n",
    "  .orderBy(\"id\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172aed35-319a-4a2c-95ce-91d3c4ed1bfd",
   "metadata": {},
   "source": [
    "### `merge` Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd08581-4d08-4c1a-8cc1-12d0b2ca18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert (merge) new data\n",
    "new_data = spark.range(0, 20)\n",
    "\n",
    "(delta_table.alias(\"old_data\")\n",
    "  .merge(\n",
    "      new_data.alias(\"new_data\"),\n",
    "      \"old_data.id = new_data.id\"\n",
    "      )\n",
    "  .whenMatchedUpdate(set = { \"id\": col(\"new_data.id\") })\n",
    "  .whenNotMatchedInsert(values = { \"id\": col(\"new_data.id\") })\n",
    "  .execute()\n",
    ")\n",
    "\n",
    "(delta_table\n",
    "  .toDF()\n",
    "  .orderBy(\"id\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e437d-93ab-4ced-839a-0f01bf568817",
   "metadata": {},
   "source": [
    "## Time travel feature of Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40cfbc6-fc4a-4f1f-9b2b-790d7170ba5c",
   "metadata": {},
   "source": [
    "### Display the entire history of the above Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c488aac-6eab-40cc-ae66-fb50b6e79e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the full history of the table\n",
    "delta_table_history = (DeltaTable\n",
    "                        .forPath(spark, \"/tmp/delta-table\")\n",
    "                        .history()\n",
    "                      )\n",
    "\n",
    "(delta_table_history\n",
    "   .select(\"version\", \"timestamp\", \"operation\", \"operationParameters\", \"operationMetrics\", \"engineInfo\")\n",
    "   .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa63ae4-11d0-4aef-9bdb-58fa76c1fd54",
   "metadata": {},
   "source": [
    "### Latest version of the Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfda73-d20a-4104-9178-040e079a92dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the full history of the table\n",
    "delta_table_history = (DeltaTable\n",
    "                        .forPath(spark, \"/tmp/delta-table\")\n",
    "                        .history()\n",
    "                      )\n",
    "\n",
    "(delta_table_history\n",
    "   .select(\"version\", \"timestamp\", \"operation\", \"operationParameters\", \"operationMetrics\", \"engineInfo\")\n",
    "   .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb30264",
   "metadata": {},
   "source": [
    "### Latest version of the Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e503d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (spark\n",
    "        .read\n",
    "        .format(\"delta\")\n",
    "        .load(\"/tmp/delta-table\")\n",
    "        .orderBy(\"id\")\n",
    "      )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cce27-2852-4fc8-b61c-747023f90658",
   "metadata": {},
   "source": [
    "### Time travel to the version `0` of the Delta Lake table using Delta Lake's history feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb43006-112b-4841-80fd-6179c802d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    "        .read\n",
    "        .format(\"delta\")\n",
    "        .option(\"versionAsOf\", 0) # we pass an option `versionAsOf` with the required version number we are interested in\n",
    "        .load(\"/tmp/delta-table\")\n",
    "        .orderBy(\"id\")\n",
    "      )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77368f4-2d51-493b-8c48-362b0ceedc38",
   "metadata": {},
   "source": [
    "### Time travel to the version `3` of the Delta Lake table using Delta Lake's  history feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa8958-237b-4be2-b397-759ea9686b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (spark\n",
    "        .read\n",
    "        .format(\"delta\")\n",
    "        .option(\"versionAsOf\", 3) # we pass an option `versionAsOf` with the required version number we are interested in\n",
    "        .load(\"/tmp/delta-table\")\n",
    "        .orderBy(\"id\")\n",
    "      )\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
