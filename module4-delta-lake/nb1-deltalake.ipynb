{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b9aecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully!\n",
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"DeltaLakeExample\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.13:4.0.1\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf2b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table created successfully!\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Starter\"), (2, \"Pro\")], [\"id\", \"plan\"])\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-tables/table2\")\n",
    "print(\"Delta table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610ae189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reading Delta Table ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/03 20:24:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   plan|\n",
      "+---+-------+\n",
      "|  1|Starter|\n",
      "|  2|    Pro|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Read the Delta table we created\n",
    "print(\"=== Reading Delta Table ===\")\n",
    "delta_df = spark.read.format(\"delta\").load(\"/tmp/delta-tables/table2\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6cd4ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Table Schema ===\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- plan: string (nullable = true)\n",
      "\n",
      "\n",
      "=== Table Count ===\n",
      "Number of records: 2\n"
     ]
    }
   ],
   "source": [
    "# 2. Check table schema and metadata\n",
    "print(\"\\n=== Table Schema ===\")\n",
    "delta_df.printSchema()\n",
    "print(\"\\n=== Table Count ===\")\n",
    "print(f\"Number of records: {delta_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727f8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Appending New Data ===\n",
      "Updated table:\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "|  4|     Basic|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Add more data (Append mode)\n",
    "print(\"\\n=== Appending New Data ===\")\n",
    "new_data = spark.createDataFrame([(3, \"Enterprise\"), (4, \"Basic\")], [\"id\", \"plan\"])\n",
    "new_data.write.format(\"delta\").mode(\"append\").save(\"/opt/spark/delta-tables/table2\")\n",
    "\n",
    "# Read updated table\n",
    "updated_df = spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\")\n",
    "print(\"Updated table:\")\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e37006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Updating Records ===\n",
      "After update:\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "|  4|     Basic|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"/opt/spark/delta-tables/table2\")\n",
    "\n",
    "# 5. Update existing records\n",
    "print(\"\\n=== Updating Records ===\")\n",
    "deltaTable.update(\n",
    "    condition=\"id = 1\",\n",
    "    set={\"plan\": \"'Starter Pro'\"}\n",
    ")\n",
    "\n",
    "# Read after update\n",
    "print(\"After update:\")\n",
    "spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ed489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Deleting Records ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/03 20:26:02 WARN DeleteCommand: Could not validate number of records due to missing statistics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After delete:\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Delete records\n",
    "print(\"\\n=== Deleting Records ===\")\n",
    "deltaTable.delete(\"id = 4\")\n",
    "\n",
    "# Read after delete\n",
    "print(\"After delete:\")\n",
    "spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09da5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Upsert/Merge Operation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/03 20:26:54 WARN MapPartitionsRDD: RDD 215 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merge:\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "|  2|   Pro Max|\n",
      "|  5|   Premium|\n",
      "+---+----------+\n",
      "\n",
      "\n",
      "=== Time Travel Queries ===\n",
      "Version 0:\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "|  4|     Basic|\n",
      "+---+----------+\n",
      "\n",
      "\n",
      "=== Final Table State ===\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "|  2|   Pro Max|\n",
      "|  5|   Premium|\n",
      "+---+----------+\n",
      "\n",
      "+-------+--------------------+---------+\n",
      "|version|           timestamp|operation|\n",
      "+-------+--------------------+---------+\n",
      "|      3|2026-02-03 20:26:...|    MERGE|\n",
      "|      2|2026-02-03 20:25:...|    MERGE|\n",
      "|      1|2026-02-03 20:25:...|   DELETE|\n",
      "|      0|2026-02-03 20:24:...|    WRITE|\n",
      "+-------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Upsert (Merge) operation\n",
    "print(\"\\n=== Upsert/Merge Operation ===\")\n",
    "merge_data = spark.createDataFrame([(2, \"Pro Max\"), (5, \"Premium\")], [\"id\", \"plan\"])\n",
    "\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    merge_data.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdate(set={\"plan\": \"source.plan\"}).whenNotMatchedInsert(values={\"id\": \"source.id\", \"plan\": \"source.plan\"}).execute()\n",
    "\n",
    "print(\"After merge:\")\n",
    "spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\").show()\n",
    "\n",
    "# 8. Time travel queries\n",
    "print(\"\\n=== Time Travel Queries ===\")\n",
    "# Read table at version 0\n",
    "print(\"Version 0:\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/opt/spark/delta-tables/table2\").show()\n",
    "\n",
    "\n",
    "print(\"\\n=== Final Table State ===\")\n",
    "spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\").show()\n",
    "deltaTable.history().select(\"version\", \"timestamp\", \"operation\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
