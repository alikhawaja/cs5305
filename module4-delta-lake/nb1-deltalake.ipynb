{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d65298d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../common_notebooks/setup_spark_connection.ipynb\n",
    "from delta.connect.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf2b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table created successfully!\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"Starter\"), (2, \"Pro\")], [\"id\", \"plan\"])\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/opt/spark/delta-tables/table2\")\n",
    "print(\"Delta table created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610ae189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reading Delta Table ===\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "|  1|   Starter|\n",
      "|  4|     Basic|\n",
      "|  2|       Pro|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Read the Delta table we created\n",
    "print(\"=== Reading Delta Table ===\")\n",
    "delta_df = spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6cd4ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Table Schema ===\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- plan: string (nullable = true)\n",
      "\n",
      "\n",
      "=== Table Count ===\n",
      "Number of records: 2\n"
     ]
    }
   ],
   "source": [
    "# 2. Check table schema and metadata\n",
    "print(\"\\n=== Table Schema ===\")\n",
    "delta_df.printSchema()\n",
    "print(\"\\n=== Table Count ===\")\n",
    "print(f\"Number of records: {delta_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727f8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Appending New Data ===\n",
      "Updated table:\n",
      "+---+----------+\n",
      "| id|      plan|\n",
      "+---+----------+\n",
      "|  3|Enterprise|\n",
      "|  1|   Starter|\n",
      "|  4|     Basic|\n",
      "|  2|       Pro|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Add more data (Append mode)\n",
    "print(\"\\n=== Appending New Data ===\")\n",
    "new_data = spark.createDataFrame([(3, \"Enterprise\"), (4, \"Basic\")], [\"id\", \"plan\"])\n",
    "new_data.write.format(\"delta\").mode(\"append\").save(\"/opt/spark/delta-tables/table2\")\n",
    "\n",
    "# Read updated table\n",
    "updated_df = spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\")\n",
    "print(\"Updated table:\")\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e37006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Updating Records ===\n"
     ]
    },
    {
     "ename": "InvalidPlanInput",
     "evalue": "No handler found for extension\n\nJVM stacktrace:\norg.apache.spark.sql.connect.common.InvalidPlanInput\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelationPlugin$3(SparkConnectPlanner.scala:253)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelationPlugin(SparkConnectPlanner.scala:253)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:235)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:74)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:314)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:225)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidPlanInput\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 5. Update existing records\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Updating Records ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdeltaTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid = 1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mStarter Pro\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Read after update\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAfter update:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\delta\\connect\\tables.py:153\u001b[39m, in \u001b[36mDeltaTable.update\u001b[39m\u001b[34m(self, condition, set)\u001b[39m\n\u001b[32m    147\u001b[39m plan = UpdateTable(\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m._plan,\n\u001b[32m    149\u001b[39m     condition,\n\u001b[32m    150\u001b[39m     assignments\n\u001b[32m    151\u001b[39m )\n\u001b[32m    152\u001b[39m df = DataFrame(plan, session=\u001b[38;5;28mself\u001b[39m._spark)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark.createDataFrame(\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\sql\\connect\\dataframe.py:1850\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mPandasDataFrameLike\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1849\u001b[39m     query = \u001b[38;5;28mself\u001b[39m._plan.to_proto(\u001b[38;5;28mself\u001b[39m._session.client)\n\u001b[32m-> \u001b[39m\u001b[32m1850\u001b[39m     pdf, ei = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plan\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1851\u001b[39m     \u001b[38;5;28mself\u001b[39m._execution_info = ei\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pdf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1004\u001b[39m, in \u001b[36mSparkConnectClient.to_pandas\u001b[39m\u001b[34m(self, plan, observations)\u001b[39m\n\u001b[32m   1000\u001b[39m (self_destruct_conf,) = \u001b[38;5;28mself\u001b[39m.get_config_with_defaults(\n\u001b[32m   1001\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1002\u001b[39m )\n\u001b[32m   1003\u001b[39m self_destruct = cast(\u001b[38;5;28mstr\u001b[39m, self_destruct_conf).lower() == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1004\u001b[39m table, schema, metrics, observed_metrics, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_destruct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_destruct\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1008\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1697\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, self_destruct)\u001b[39m\n\u001b[32m   1694\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1697\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1674\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, progress)\u001b[39m\n\u001b[32m   1672\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1673\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1674\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1982\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1980\u001b[39m     \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1981\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m1982\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1983\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m   1984\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2066\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   2057\u001b[39m                 logger.info(\n\u001b[32m   2058\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mDisabling plan compression for the session due to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2059\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mCONNECT_INVALID_PLAN.CANNOT_PARSE error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2060\u001b[39m                 )\n\u001b[32m   2061\u001b[39m                 \u001b[38;5;28mself\u001b[39m._plan_compression_threshold, \u001b[38;5;28mself\u001b[39m._plan_compression_algorithm = (\n\u001b[32m   2062\u001b[39m                     -\u001b[32m1\u001b[39m,\n\u001b[32m   2063\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mNONE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2064\u001b[39m                 )\n\u001b[32m-> \u001b[39m\u001b[32m2066\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   2067\u001b[39m                 info,\n\u001b[32m   2068\u001b[39m                 status.message,\n\u001b[32m   2069\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   2070\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   2071\u001b[39m                 status_code,\n\u001b[32m   2072\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2074\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[32m   2075\u001b[39m         message=status.message,\n\u001b[32m   2076\u001b[39m         grpc_status_code=status_code,\n\u001b[32m   2077\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2078\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mInvalidPlanInput\u001b[39m: No handler found for extension\n\nJVM stacktrace:\norg.apache.spark.sql.connect.common.InvalidPlanInput\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelationPlugin$3(SparkConnectPlanner.scala:253)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelationPlugin(SparkConnectPlanner.scala:253)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:235)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:74)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:314)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:225)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"/opt/spark/delta-tables/table2\")\n",
    "\n",
    "# 5. Update existing records\n",
    "print(\"\\n=== Updating Records ===\")\n",
    "deltaTable.update(\n",
    "    condition=\"id = 1\",\n",
    "    set={\"plan\": \"'Starter Pro'\"}\n",
    ")\n",
    "\n",
    "# Read after update\n",
    "print(\"After update:\")\n",
    "spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Delete records\n",
    "print(\"\\n=== Deleting Records ===\")\n",
    "deltaTable.delete(\"id = 4\")\n",
    "\n",
    "# Read after delete\n",
    "print(\"After delete:\")\n",
    "spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da5ef7",
   "metadata": {
    "vscode": {
     "languageId": "lua"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Upsert (Merge) operation\n",
    "print(\"\\n=== Upsert/Merge Operation ===\")\n",
    "merge_data = spark.createDataFrame([(2, \"Pro Max\"), (5, \"Premium\")], [\"id\", \"plan\"])\n",
    "\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    merge_data.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdate(set={\"plan\": \"source.plan\"}).whenNotMatchedInsert(values={\"id\": \"source.id\", \"plan\": \"source.plan\"}).execute()\n",
    "\n",
    "print(\"After merge:\")\n",
    "spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\").show()\n",
    "\n",
    "# 8. Time travel queries\n",
    "print(\"\\n=== Time Travel Queries ===\")\n",
    "# Read table at version 0\n",
    "print(\"Version 0:\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/opt/spark/delta-tables/table2\").show()\n",
    "\n",
    "# 9. Vacuum old files (cleanup)\n",
    "print(\"\\n=== Vacuum Operation ===\")\n",
    "# Note: Vacuum removes files older than retention period\n",
    "deltaTable.vacuum(0)  # 0 hours for demo (use 168 hours/7 days in production)\n",
    "\n",
    "print(\"\\n=== Final Table State ===\")\n",
    "spark.read.format(\"delta\").load(\"/opt/spark/delta-tables/table2\").show()\n",
    "deltaTable.history().select(\"version\", \"timestamp\", \"operation\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
