{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "631b6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run nb0.spark-session.ipynb\n",
    "from pyspark.sql.functions import col, lit, when, upper, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f805465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created (Transformations recorded).\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Laptop\", \"Electronics\", 1200, 5),\n",
    "    (2, \"Smartphone\", \"Electronics\", 800, 10),\n",
    "    (3, \"Coffee Maker\", \"Home\", 150, 20),\n",
    "    (4, \"Desk Chair\", \"Furniture\", 250, 15),\n",
    "    (5, \"Monitor\", \"Electronics\", 300, 8)\n",
    "]\n",
    "\n",
    "schema = [\"id\", \"product\", \"category\", \"price\", \"stock\"]\n",
    "\n",
    "# Transformation: Creating the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Note: No computation has happened yet. Spark only knows the schema.\n",
    "print(\"DataFrame created (Transformations recorded).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1049ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "LocalTableScan [id#926L, product#934, category#928, price#929L, stock#930L, total_value#932L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Filter for Electronics\n",
    "# 2. Add a 'total_value' column (price * stock)\n",
    "# 3. Uppercase the product name\n",
    "transformed_df = df.filter(col(\"category\") == \"Electronics\") \\\n",
    "                   .withColumn(\"total_value\", col(\"price\") * col(\"stock\")) \\\n",
    "                   .withColumn(\"product\", upper(col(\"product\")))\n",
    "\n",
    "# This is still lazy! Let's look at the execution plan Spark has built:\n",
    "transformed_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5d015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Action: show()\n",
      "+---+----------+-----------+-----+-----+-----------+\n",
      "| id|   product|   category|price|stock|total_value|\n",
      "+---+----------+-----------+-----+-----+-----------+\n",
      "|  1|    LAPTOP|Electronics| 1200|    5|       6000|\n",
      "|  2|SMARTPHONE|Electronics|  800|   10|       8000|\n",
      "|  5|   MONITOR|Electronics|  300|    8|       2400|\n",
      "+---+----------+-----------+-----+-----+-----------+\n",
      "\n",
      "Total Electronics items: 3\n"
     ]
    }
   ],
   "source": [
    "# Action: show()\n",
    "print(\"Executing Action: show()\")\n",
    "transformed_df.show()\n",
    "\n",
    "# Action: count()\n",
    "print(f\"Total Electronics items: {transformed_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9139d9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|   category|        avg_price|\n",
      "+-----------+-----------------+\n",
      "|Electronics|766.6666666666666|\n",
      "|  Furniture|            250.0|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chaining complex logic\n",
    "final_analysis = df.groupBy(\"category\") \\\n",
    "    .agg(avg(\"price\").alias(\"avg_price\")) \\\n",
    "    .filter(col(\"avg_price\") > 200)\n",
    "\n",
    "# Trigger Action\n",
    "final_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b02a9ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Filter (isnotnull(avg_price#1024) AND (avg_price#1024 > 200.0))\n",
      "   +- HashAggregate(keys=[category#1027], functions=[avg(price#1028L)])\n",
      "      +- Exchange hashpartitioning(category#1027, 200), ENSURE_REQUIREMENTS, [plan_id=817]\n",
      "         +- HashAggregate(keys=[category#1027], functions=[partial_avg(price#1028L)])\n",
      "            +- LocalTableScan [category#1027, price#1028L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_analysis.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834d9f0",
   "metadata": {},
   "source": [
    "- **HashAggregate**: Groups input rows by hash-based keys and computes aggregations in-memory for each partition.  \n",
    "- **Exchange HashPartitioning**: Redistributes data across executors by hashing partition keys so that rows sharing a key land on the same partition for downstream operations.  \n",
    "- **LocalTableScan**: Reads already available in-memory data within the executor without additional shuffles or I/O, serving as the starting point for local query fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9a93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
